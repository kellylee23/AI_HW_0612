{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### vscode를 이용하여 코드를 작성하였습니다.<br>202202665 이은서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### **기존모델 데이터 이용**\n",
    "##### 데이터셋 : 'cats_vs_dogs'\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[ 통제변인 ]</h4>\n",
    "1. Sequencial 이용 <br>\n",
    "2. epoch = 10 <br>\n",
    "3. activation : sigmoid 함수 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>[ 조작변인 ]</h4> : optimizer변경 및  batch_size변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***optimizer에 대한 이해***\n",
    "\n",
    "1. optimizer란?\n",
    "    * 딥러닝 모델의 학습 과정에서 손실 함수(loss function)를 최소화하기 위해 모델의 가중치(weights)를 조정하는 알고리즘. <br>\n",
    "    * 옵티마이저는 모델이 데이터를 더 잘 예측할 수 있도록 가중치를 업데이트하는 방법을 결정한다.\n",
    "\n",
    "\n",
    "2. 주요 optimizer 종류\n",
    "    * SGD(Stochastic Gradient Descent)\n",
    "        * 학습 데이터의 작은 무작위 배치에 대해 경사 하강법을 적용한다.\n",
    "        * 장점 : 간단하고 구현이 쉽고, 자주 사용되는 방법이다.\n",
    "        * 단점 : 학습속도가 느리고, local minima에 빠질 수 있다.\n",
    "    * Momentum\n",
    "        * SGD의 변형으로, 이전 기울기(gradient)의 방향을 고려하여 현재 기울기를 업데이트한다.\n",
    "        * 장점 : 진동을 줄이고 더 빠르게 수렴할 수 있다.\n",
    "        * 단점 : 하이퍼파라미터(momentum coefficient) 조정이 필요하다.\n",
    "    * RMSprop\n",
    "        * 기울기의 이동 평균을 사용하여 학습률을 조정한다.\n",
    "        * 장점 : 학습률이 일정하게 유지되어 더 안정적인 학습이 가능하다.\n",
    "        * 단점 : 하이퍼파라미터(beta) 조정이 필요하다.\n",
    "    * Adam\n",
    "        * Momentum과 RMSprop을 결합한 방법으로, 학습률을 개별적으로 조정하면서도 기울기의 이동 평균을 사용한다.\n",
    "        * 장점 : 빠르고 효율적이며 대부분의 문제에서 좋은 성능을 보인다.\n",
    "    * Nadam\n",
    "        * Adam에 Nesterov 가속 경사법(Nesterov accelerated gradient, NAG)을 추가한 방법이다.\n",
    "        * 장점 : 더 빠른 수렴과 조금 일반적으로 더 나은 성능을 제공한다.\n",
    "        * 단점 : Adam보다 조금 더 복잡하다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 모델컴파일 (1) - 기존모델\n",
    "    1. loss : binary_crossentropy\n",
    "    2. optimizer : **SGD(Stochastic Gradient Descent)**\n",
    "    3. metrics : accuracy\n",
    "    4. batch_size = 32\n",
    "\n",
    "- 모델컴파일 (2)\n",
    "    1. loss : binary_crossentropy\n",
    "    2. optimizer : **RMSProp**\n",
    "    3. metrics : accuracy\n",
    "    4. batch_size = 32\n",
    "\n",
    "- 모델컴파일 (3)\n",
    "    1. loss : binary_crossentropy\n",
    "    2. optimizer : **Adam**\n",
    "    3. metrics : accuracy\n",
    "    4. batch_size = 32\n",
    "\n",
    "- 모델컴파일 (4-1)\n",
    "    1. loss : binary_crossentropy\n",
    "    2. optimizer : **Nadam**\n",
    "    3. metrics : accuracy\n",
    "    4. batch_size = 32\n",
    "\n",
    "- 모델컴파일 (4-2)\n",
    "    1. loss : binary_crossentropy\n",
    "    2. optimizer : **Nadam**\n",
    "    3. metrics : accuracy\n",
    "    4. batch_size = 16\n",
    "        // 배치 사이즈를 줄이는 이유는 더 자주 가중치가 업데이트 되기 때문이다. 더 세밀한 업데이트를 가능하게 하여, 복잡한 손실 곡면에서 더 효과적으로 최적점을 찾을 수 있게 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***<모델 컴파일 (1)>***\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=SGD(),metrics=['accuracy'])\n",
    "\n",
    "batch_size =32\n",
    "\n",
    "<(1)의 결과><br>\n",
    "**Accuracy: 0.7927772998809814**  \n",
    "\n",
    "//SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***<모델 컴파일 (2)>***\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "batch_size =32\n",
    "\n",
    "<(2)의 결과><br>\n",
    "**Accuracy: 0.8323301672935486**\n",
    "\n",
    "\n",
    "//RMSprop\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***<모델 컴파일 (3)>***\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "batch_size =32\n",
    "\n",
    "<(3)의 결과><br>\n",
    "**Accuracy: 0.8327600955963135**\n",
    "\n",
    "\n",
    " //Adam\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***<모델 컴파일 (4-1)>***\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Nadam(),metrics=['accuracy'])\n",
    "\n",
    "batch_size =32\n",
    "\n",
    "<(4-1)의 결과><br>\n",
    "**Accuracy: 0.8314703106880188**\n",
    "\n",
    "\n",
    " //Nadam(32)\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***<모델 컴파일 (4-2)>***\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=Nadam(),metrics=['accuracy'])\n",
    "\n",
    "batch_size =16\n",
    "\n",
    "<(4-2)의 결과><br>\n",
    "**Accuracy: 0.8364144563674927**\n",
    "\n",
    "\n",
    " //Nadam(16)\n",
    "\n",
    " ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 : 결과에 따라 한 개의 배치를 불러와서 강아지인지, 고양이 인지 예측하기.\n",
    "\n",
    "ex) 모델 컴파일 (4-1)의 예측으로는 \n",
    "(readme참고)\n",
    "\n",
    "이러한 사진 속 동물이 강아지인지 고양이인지에 대한 결과가 예측된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌟<br>[결과]\n",
    "\n",
    "이 딥러닝 모델은 'cats_vs dogs_'의 데이터셋을 불러와 모델의 성능을 개선시키고자, optimizer를 변경한 것이다.\n",
    "\n",
    "optimizer를 변경 하였을 때,\n",
    "\n",
    "***SGD < Nadam(32) < RMSporp < Adam < Nadam(16)***\n",
    "\n",
    "순서대로 정확도가 높아짐을 확인할 수 있다.<br> \n",
    "<br>\n",
    "📌\n",
    "\n",
    "따라서, \n",
    "\n",
    "점차 성능이 개선되었음을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
